[
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html",
    "href": "posts/Probability Theory and Random Variables/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "A Continuous Probability Distribution"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#introduction",
    "href": "posts/Probability Theory and Random Variables/index.html#introduction",
    "title": "Probability Theory and Random Variables",
    "section": "Introduction",
    "text": "Introduction\nUnderstanding probability and statistics is, whether graduate students like it or not, essential to the study of machine learning and artificial intelligence. These fields are inextricably linked and any aspiring AI researcher or data scientist should seek to have a strong foundation in the former two. Computers, and the various algorithms we have developed on them, operate on data, which is a fairly ambiguous term here. Data is, in short, a quantity or collection of different values which may exist discretely or continuously. As computers have become exceedingly fast and able to store seemingly endless amounts of information, the absolute volume of data in our world is immense. Data can be as simple as a number or as complex as images and videos. Data may also be intentionally (or unintentionally) generated in a poor manner and as such may not be indicative of anything. As such, any subset of the general data that exists, may not necessarily reflect reality. For example, a misleading dataset might cause one to come to the wrong conclusions about, say, climate change, whereas if you looked at a larger, more inclusive collection of data, a different conclusion may be reached (to be diplomatic, I will not disclose which is which here).\nStatisticians and probability theorists have over the centuries developed methods for assessing the quality of and producing data. The work of these great men and women has laid the groundwork for the wonderful fields of probability and statistics. Interacting with data requires that we, as computer scientists, have an understanding of what makes data reliable or “good” in some quality. To be able to accurately assess data and develop intelligent machines to process and reveal further insights from it, it is crucial to study the fields of probability and statistics. In this post, I will cover several fundamental concepts in the space of probability theory with an emphasis on their relation to machine learning. I assume my readers have some familiarity with probability theory in general and much of the this post will be review."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#probability-and-statistics",
    "href": "posts/Probability Theory and Random Variables/index.html#probability-and-statistics",
    "title": "Probability Theory and Random Variables",
    "section": "Probability and Statistics",
    "text": "Probability and Statistics\nProbability is, put simply, the branch of mathematics concerning itself with randomness and its applications in determining the likelihood of events. If you study a technical field, you have likely taken classes or independently learned many concepts within probability theory. Statistics is a broader concept than probability theory because it involves more than just mathematics. Often, the discipline of statistics also incorporates things such as the collection, presentation, and organization of data. For the sake of brevity, I will discuss primarily the mathematical side of statistics - the analysis and interpretation of data. Some important applications of mathematical statisticas are descriptive analysis (like determining means, medians, and modes) and the generation of statistical models (which leads to inferential statistics - a core concept in machine learning). Let us begin with a simple example: flipping a coin. While you could argue flipping a coin is dependent on how one flips it, if the coin is tampered with, or fate, let us assume it is a perfect, idealized coin that has an entirely random outcome. We expect half the time to see heads as the outcome and the other half we would tails. The following Python code simulates 10 (idealized) coin flips and plots their results:\n\n\nCode\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef flipCoin():\n    coinFlip = random.randint(0, 1)\n    if (coinFlip == 0):\n        return \"heads\"\n    else:\n        return \"tails\"\n\nmaxFlips = 10\ndeltaList = []\nrecordList = []\nnumHeads = 0\nnumTails = 0\nnumFlips = 0\n\nfor i in range(maxFlips):\n    recordList.append(flipCoin())\n    if recordList[i] == \"heads\":\n        numHeads += 1\n    else:\n        numTails += 1\n    numFlips += 1\n    deltaList.append((numHeads - numTails) / numFlips)\n\nplt.plot(deltaList)\nplt.grid()\nplt.axis([1, maxFlips - 1, -1, 1])\nplt.xticks(np.arange(len(deltaList)), np.arange(1, len(deltaList) + 1))\nplt.xlabel(\"Number of Coin Flips\")\nplt.ylabel(\"Normalized Difference [(Heads - Tails) / Number of Flips]\")\nplt.show()\n\n\n\n\n\nFigure 1: Results from 10 Coin Flips\n\n\n\n\nThe y-axis of the graph above shows the difference between the number of heads outcomes and the number of tails outcomes normalized by the total number of flips. This gives a relative measure of how skewed the current proportion of outcomes is relative to the expected 50-50 outcome. I encourage you to try running this code in your own Jupyter environment several times. If run multiple times, the above code will not repeatedly show 0.00 by the final flip, though you may notice a general tendency for the y-value of the plot to tend toward 0.00. As we increase the number of coin flips, this trend becomes much more apparent:\n\n\nCode\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef flipCoin():\n    coinFlip = random.randint(0, 1)\n    if (coinFlip == 0):\n        return \"heads\"\n    else:\n        return \"tails\"\n\nmaxFlips = 1000\ndeltaList = []\nrecordList = []\nnumHeads = 0\nnumTails = 0\nnumFlips = 0\n\nfor i in range(maxFlips):\n    recordList.append(flipCoin())\n    if recordList[i] == \"heads\":\n        numHeads += 1\n    else:\n        numTails += 1\n    numFlips += 1\n    deltaList.append((numHeads - numTails) / numFlips)\n\nplt.plot(deltaList)\nplt.axis([1, maxFlips - 1, -1, 1])\nplt.xticks(plt.xticks()[0][1::1])\nplt.xlabel(\"Number of Coin Flips\")\nplt.ylabel(\"Normalized Difference [(Heads - Tails) / Number of Flips]\")\nplt.show()\n\n\n\n\n\nFigure 2: Results from 1000 Coin Flips\n\n\n\n\nIf you run the provided code multiple times, it is exceedingly apparent that with more coin flips, the normalized difference between the number of heads outcomes and tails outcomes trends toward 0.00. If a coin is flipped just 10 times, even a single flip difference seems significant. As the number of flips increases, however, we see that the effect of randomness seen in the outcome diminishes. The effect seen on the y-axis of the above graphs roughly correlates to the following:\n\\[\nX(c) = \\begin{cases}\n            \\phantom{-}1, & \\text{if c = heads} \\\\\n            -1, & \\text{if c = tails} \\\\\n        \\end{cases}\n\\]\nThe variable \\(X\\) introduced in the above equation is called a random variable. A random variable is essentially a quantity that is dependant on a random event. In this case, the random event that the variable \\(X\\) depends on is the coin flip. To be more specific, \\(X\\) is a discrete random variable. Discrete random variables are random variables that may take on one of a finite or countably infinite set of values. There is a “distance” between a value and its nearest neighbor and that distance is nonzero. This contrasts with what are known as continuous random variables, which may take on any single value from an uncountable set.\nSince a coin flip result is a discrete random variable, we can assign a probability to each outcome of a coin flip. There is a 50% chance that a flip lands on heads, and a 50% chance that it lands on tails. We would say then that:\n\\[\np(heads) = \\frac{1}{2} \\hspace{3mm} \\text{and} \\hspace{3mm} p(tails) = \\frac{1}{2}\n\\]\nNotice that the probabilities of each of the above events happening sum to 1. For any finite set of possible events, the individual probabilities of each event occurring have a sum of 1. This makes intuitive sense: if an event can happen, there is some quantifiable chance of it occurring, and if an event cannot happen, its probability is 0! In the example of the coin flip, the equal likelihood of heads and tails occurring is what is ultimately responsible for the damping behavior of Figure 2."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html#probability-statistics-and-machine-learning",
    "href": "posts/Probability Theory and Random Variables/index.html#probability-statistics-and-machine-learning",
    "title": "Probability Theory and Random Variables",
    "section": "Probability, Statistics, and Machine Learning",
    "text": "Probability, Statistics, and Machine Learning\nProbability and statistics are in and of themselves immensely complex and widely practiced disciplines with people who devote their lives to their study. Machine learning, as a discipline, seeks to apply the above principles (and many more) to computers such that they may be able to learn from data. As I hinted at earlier, inferential statistics has a large application area in machine learning, where computer algorithms are used to determine predictive patterns from relatively small sample sizes and use this to infer information about the population (or dataset) that the sample was taken from. Linear regression, which many engineers and computer scientists learn of in, say, an introductory linear algebrea class, is a simple and well-studied machine learning algorithm. Besides this, many statistical concepts like means, errors, and variances all have applications in the development and evaluation of machine learning models.\nMachine learning is about giving a computer the tools it needs to discover things for itself and do so correctly. Understanding how statistical models are made to accurately create associations between data allows us to better develop and evaluate machine learning models."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clusters of Data"
  },
  {
    "objectID": "posts/Clustering/index.html#clustering-algorithms",
    "href": "posts/Clustering/index.html#clustering-algorithms",
    "title": "Clustering",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\nIn many data science applications, it is necessary to discover relationships between data points such that we might identify different characteristics about the structure of our data. Distances between data points and the density of different ‘areas’ of our data may have value in determining patterns that would otherwise go unnocticed. We refer to the task of identifying related or ‘close’ subsets of data as clustering. Clustering does not attempt to make predictions about data. In fact, clustering is an unsupervised maching learning paradigm, which means it does not require labelled data to identify relationships within it. Clustering depends only on the data itself and does not require human supervision to discover patterns or other insights about the data. It is important to note, however, that clustering data can provide new insights to data consumers who may then use the results of cluster analysis to make predictions or train other machine learning models."
  },
  {
    "objectID": "posts/Clustering/index.html#beans",
    "href": "posts/Clustering/index.html#beans",
    "title": "Clustering",
    "section": "Beans",
    "text": "Beans\nFor this post, I will be utilizing a dataset containing data about beans. You may find the data used in this post here. Essentially, the dataset breaks down various measurements taken of seven different types of beans. The two we will be using to visualize and cluster our data are the major and minor axis lengths, which are measured in pixels (px) in this dataset. We could use other features as well, but limiting it to two allows us to clearly visualize the dataset and make assessments. Below, we plot the data:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"Beans.csv\")\n\nprint(df.head());\n\nmajor_axis = df[\"MajorAxisLength\"]\nminor_axis = df[\"MinorAxisLength\"]\n\nplt.scatter(major_axis, minor_axis, s=1);\nplt.xlabel(\"Major Axis Length (px)\");\nplt.ylabel(\"Minor Axis Length (px)\");\n\n\n    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n0  28395    610.291       208.178117       173.888747      1.197191   \n1  28734    638.018       200.524796       182.734419      1.097356   \n2  29380    624.110       212.826130       175.931143      1.209713   \n3  30008    645.884       210.557999       182.516516      1.153638   \n4  30140    620.134       201.847882       190.279279      1.060798   \n\n   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n1      0.411785       29172     191.272751  0.783968  0.984986   0.887034   \n2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n\n   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  \n\n\n\n\n\nFigure 1: Bean Data\n\n\n\n\nClearly, we can see the data appears to have more or less well-defined clusters. A qualitative examination of the graph shows us at least five of the seven clusters, with two of the remaining clusters likely obfuscated in the large blobs in thie lower left quadrant of the plot. The algorithm we will use to cluster our data in the next code segment is known as k-means. A k-means algorithm takes a set of data and attempts to cluster data around the nearest mean, also known as a cluster centroid. These clusters have minimized within-cluster variances (based on squared Euclidian distances). Included in the below code are two metrics that tell us information about the quality of our clustering, computed after running our model:\n\n\nCode\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom warnings import filterwarnings\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import silhouette_score\n\nfilterwarnings('ignore')\n\nX = df[['MajorAxisLength', 'MinorAxisLength']].copy()\n\nlabel_encoder = LabelEncoder()\nbean_type = df[\"Class\"]\nnumeric_labels = label_encoder.fit_transform(bean_type)\n\nk = 7\nkmeans = MiniBatchKMeans(n_clusters=k, random_state=55)\ny_pred = kmeans.fit_predict(X)\n\ncentroids = kmeans.cluster_centers_\n\nmins = X.min(axis=0) - 0.1\nmaxs = X.max(axis=0) + 0.1\n\nxx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], 1000),\n                     np.linspace(mins[1], maxs[1], 1000))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap='Pastel2')\nplt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors='k')\n\nplt.scatter(major_axis, minor_axis, c=numeric_labels, s=1);\nplt.xlabel(\"Major Axis Length (px)\");\nplt.ylabel(\"Minor Axis Length (px)\");\n\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='o', s=25, linewidths=6, color='w', zorder=10)\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=2, linewidths=10, color='k', zorder=11, alpha=1)\n\nsilhouette_avg = silhouette_score(X, kmeans.labels_);\nprint(\"Silhouette Score:\", silhouette_avg);\n\ncalinski_harabasz_score = calinski_harabasz_score(X, kmeans.labels_);\nprint(\"Calinski-Harabasz Index:\", calinski_harabasz_score);\n\nplt.show()\n\n\nSilhouette Score: 0.3992331677981161\nCalinski-Harabasz Index: 34239.3188491691\n\n\n\n\n\nFigure 2: Clustered Bean Data\n\n\n\n\nEach of the seven beans has been identified in the above graph with a distinct color. The clustered colors represent the actual clusters of data (if perfectly clustered by color). The clustering algorithm used above appears to have determined quite accurately the centroids of the seven clusters predicted to exist in the figure. The centroids, marked by a circle with a black cross, are placed more or less in the center of each of the seven clusters. Two model metrics shown above are the Silhouette Score and the Calinski-Harabasz Index. The Silhouette Score gives a measure of how well-matched our data is to other data points within their clusters as opposed to how well-matched it is to data outside of their clusters. Ranging from -1 to 1, with 1 being well-matched within cluster, and -1 representing poor matching, the score produced from the above clustering shows that the data is relatively well-matched within clusters. I would hazard a guess that the blob made it somewhat difficult for our data to be perfectly clustered into seven groups. The Calinski-Harabasz Index similarly identifies how well-clustered the data is as a ratio between the sum of between-cluster dispersion and within-cluster dispersion. Essentailly, the larger the index, the more defined the clusters are.\nIt is important to note that neither of these metrics depend upon labels known ahead of time and are solely dependent on the dataset itself (the major and minor axis lengths) and the results of the clustering algorithm. This should further accentuate the nature of k-means as an unsupervised machine learning algorithm."
  },
  {
    "objectID": "posts/Clustering/index.html#clustering-in-higher-dimensions",
    "href": "posts/Clustering/index.html#clustering-in-higher-dimensions",
    "title": "Clustering",
    "section": "Clustering in Higher Dimensions",
    "text": "Clustering in Higher Dimensions\nAs an addition to the above discussion on clustering, it is important to note that clustering may be performed on data sets with high numbers of important features, even more so than can be represented in traditional two or three dimensional plots. In many machine learning scenarios, it is important to consider many features, often more than can be easily visualized in low-dimensional spaces (which are the only ones we can graph and comprehend). There exist different methods for clustering data in these higher dimension spaces, often with tradeoffs associated with each method. Projection-based clustering methods tend to be the best at determining high-dimensional distance or density structures. The best of these methods construct graphs and use a (hopefully) familiar shortest path algorithm, Dijkstra’s algorithm, to compute the necessary quantities for clustering. It is possible that including other quantities from the bean dataset used above may have led to better clustering of our data."
  },
  {
    "objectID": "posts/Anomaly and Outlier Detection/index.html",
    "href": "posts/Anomaly and Outlier Detection/index.html",
    "title": "Anomaly and Outlier Detection",
    "section": "",
    "text": "Noise Identified in a Dataset"
  },
  {
    "objectID": "posts/Anomaly and Outlier Detection/index.html#anomalies-and-outliers",
    "href": "posts/Anomaly and Outlier Detection/index.html#anomalies-and-outliers",
    "title": "Anomaly and Outlier Detection",
    "section": "Anomalies and Outliers",
    "text": "Anomalies and Outliers\nIn this post, I will cover a use case for machine learning algorithms known as anomaly (or outlier) detection. The basic principle is this: we can use ML methods to detect data (events, items, etc) that may be rare, suspicious, or otherwise deviant from the standard pattern of a given dataset. This has potential application in a large variety of fields, ranging from defect detection in manufacturing, processing of data, or, as I will show in this post, credit card fraud detection. Anomaly and outlier detection does not represent a subdiscipline of machine learning like clustering, classification, or neural networks do, but rather an area of application of these models. There exist neural network, clustering, and ensemble methods that may all be used to perform detection of deviant data. I will demonstrate how you can use k-means to cluster data and assign anomalous data a flag that says it represents an outlier."
  },
  {
    "objectID": "posts/Anomaly and Outlier Detection/index.html#credit-card-fraud-and-detection",
    "href": "posts/Anomaly and Outlier Detection/index.html#credit-card-fraud-and-detection",
    "title": "Anomaly and Outlier Detection",
    "section": "Credit Card Fraud and Detection",
    "text": "Credit Card Fraud and Detection\nWhile we all wish bad people did not exist in our world, it is a sad fact of life that we must deal with. Credit card fraud has plagued the United States practically since the inception of the credit card, and as such it has been a constant struggle to stay ahead of bad actors that try to commit fraud. Anomaly detection has been used to help identify suspicious activity and prevent future fraud from occurring or at least stop current fraud in progress.\nThe dataset used in this example can be found here. To begin, I will take the two CSV files provided and merge them together before plotting various different feature relationships. The code used is shown below:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\n\ncards = pd.read_csv(\"CreditCards.csv\")\ntransactions = pd.read_csv(\"CC_Transactions.csv\")\n\ndf = transactions.merge(cards, on='credit_card')\n\nstate_counts = Counter(df['state'])\nstate_counts = sorted(state_counts.items(), key=lambda x: x[1], reverse=True)\nstates, counts = zip(*state_counts)\n\nfig, axes = plt.subplots(3, 1, tight_layout=True)\n\nfig.set_size_inches(h=10, w=8);\n\nsns.scatterplot(data=df, x='credit_card_limit', y='transaction_dollar_amount', ax=axes[0]);\naxes[0].set_xlabel(\"Credit Card Limit ($)\");\naxes[0].set_ylabel(\"Transaction Amount ($)\");\n\nsns.histplot(df['transaction_dollar_amount'], kde=True, ax=axes[1]);\naxes[1].set_xlabel(\"Transaction Amount ($)\");\naxes[1].set_ylabel(\"Number of Transactions\");\n\nsns.barplot(x=states, y=counts, ax=axes[2]);\naxes[2].set_xlabel(\"State\");\naxes[2].set_ylabel(\"Number of Transactions\");\naxes[2].set_xticklabels(states, rotation=45);\n\n\n\n\n\nFigure 1: Credit Card Limit vs. Transaction Amount\n\n\n\n\nImmediately, it is apparent there are outlier clumps of data that may represent fraudulent activity. We see in the first and second plots that in the higher end transaction amount range, there is a noticable jump in transaction volume. From the first plot, we can see that this is relatively uniform across credit card limits. Potentially, we might see higher purchase amounts as credit limits increase (maybe that assumes responsible consumers, however, which is never a good assumption). Still, the uniformness across card limits appears immediately suspicious. In the third plot, we see another unusual relationship in the number of transactions by state. New Hampshire, a relatively small state in the United States, has an unusually high volume of transactions. Perhaps this is a coincidence and simply a function of the dataset used, but either way, a detection algorithm can make this determination. In the following code segment, I will perform a k-means clustering of the data and predict a label based on which cluster each data entry belongs to. Based on the above graphs, I will assume we have two clusters of data - fraudulent transactions and normal transactions. The cluster label will then be used to determine whether or not the data is anomalous (and therefore fraudulent). I then add the label to the original dataframe for plot generation. Below is the code:\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize k-means model and scaler\nkmeans = KMeans(n_clusters=2, random_state=213, n_init=10)\n\nscaler = StandardScaler()\n\nimportant_features = ['transaction_dollar_amount', 'Long', 'Lat', 'credit_card_limit']\n\n# Create a second dataframe (helps make plots prettier)\nmodified_df = df.copy()\nmodified_df[important_features] = scaler.fit_transform(modified_df[important_features])\n\n# Fit model and predict fraudulent data\nkmeans.fit(modified_df[important_features])\nmodified_df['cluster_label'] = kmeans.predict(modified_df[important_features])\n\nfraud_label = modified_df.groupby('cluster_label')['transaction_dollar_amount'].mean().idxmax()\nmodified_df['Fraudulent'] = modified_df['cluster_label'].apply(lambda x: 1 if x == fraud_label else 0)\n\n# Apply the resulting features from the outlier/anomaly detection to the original dataframe\ndf['Fraudulent'], df['cluster_label'] = modified_df['Fraudulent'], modified_df['cluster_label']\n\ndf.replace({'Fraudulent': {0: 'No', 1: 'Yes'}}, inplace=True)\n\nexplode = [0, 0.2]\n\nfraud_count = df['Fraudulent'].value_counts()\n\nplt.figure(figsize=(8, 7));\nplt.pie(fraud_count, labels=fraud_count.values, explode=explode);\nplt.legend(['Not Fraudulent', 'Fraudulent']);\nplt.show()\n\n\n\n\n\nFigure 2: Results from Anomaly Detection Analysis\n\n\n\n\nWe have found that a small percentage of the data appears fraudulent based on the k-means clustering. In some n-dimensional space, where n is the number of features we have, there exists a cluster of data points that represent the fraudulent data. The original three graphs above are ultimately projections of this data from the n-dimensional space onto a two-dimensional plots of features. The k-means model used above discovered this cluster, which would be exceptionally hard for humans to imagine, but apparently not difficult for a computer to perform analysis on. Below, I have generated three more plots that roughly reflect the original three from above, but with the fraudulent data labelled:\n\n\nCode\nfig, axes = plt.subplots(3, 1, tight_layout=True)\n\nfig.set_size_inches(h=10, w=8);\n\nsns.scatterplot(data=df, x='credit_card_limit', y='transaction_dollar_amount', hue='Fraudulent', ax=axes[0]);\naxes[0].set_xlabel(\"Credit Card Limit ($)\");\naxes[0].set_ylabel(\"Transaction Amount ($)\");\n\nsns.violinplot(data=df, x='transaction_dollar_amount', y='Fraudulent', hue='Fraudulent', orient='h', ax=axes[1]);\naxes[1].set_xlabel(\"Transaction Amount ($)\");\naxes[1].set_ylabel(\"Fraudulent?\");\n\ncopy_for_plot = df.copy()\nstate_counts = copy_for_plot['state'].value_counts()\ncopy_for_plot = copy_for_plot.sort_values(by='state', key=lambda x: x.map(state_counts), ascending=False)\n\nsns.countplot(data=copy_for_plot, x='state', hue='Fraudulent', ax=axes[2]);\naxes[2].set_xlabel(\"State\");\naxes[2].set_ylabel(\"Number of Transactions\");\naxes[2].set_xticklabels(states, rotation=45);\n\n\n\n\n\nFigure 3: Original Graphs with Fraud Labels\n\n\n\n\nThe model identified the transactions that appeared fraudulent. It seems to agree with what intuition might suggest: high transaction amounts, regardless of credit limit, flagged as fraudulent with high frequency. Besides this, we do not see much association between state and and fraudulent transactions beyond what is expected (more fraud where there are more transactions). Below, I will display some city data and metrics associated with the clustering of the data:\n\n\n\nCode\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_avg = silhouette_score(modified_df[important_features], kmeans.labels_);\nprint(\"Silhouette Score:\", silhouette_avg);\n\ncalinski_harabasz_score = calinski_harabasz_score(modified_df[important_features], kmeans.labels_);\nprint(\"Calinski-Harabasz Index:\", calinski_harabasz_score);\n\nprint(\"\\n\")\n\nfraud_by_city = df[df['Fraudulent'] == 'Yes'].groupby('city').size().sort_values(ascending=False)\nprint(\"Fraudulent Transactions by City:\")\nprint(fraud_by_city)\n\n\nSilhouette Score: 0.7354083037666868\nCalinski-Harabasz Index: 74193.94579757533\n\n\nFraudulent Transactions by City:\ncity\nWashington    2059\nHouston        384\nNew York       292\nEl Paso        233\nDallas         193\n              ... \nMontgomery       1\nNew Haven        1\nWichita          1\nRoanoke          1\nBaltimore        1\nLength: 121, dtype: int64\n\n\n\nBased on the Silhouette Score and Calinski-Harabasz Index of our clustering algorithm indicate that our data has formed unique clusters (between the fraudulent and normal data points). Silhouette Score shows that each data point is matched well with other data points in-cluster while being poorly matched with data points outside of it (which shows good clustering). The Calinski-Harabasz Index shows that the clusters are well defined, which our graphics above allow us to confirm qualitatively.\nAs a side note, it is interesting to see that Washington DC has the highest number of frauduluent transactions in our dataset. Were we to apply a classifier to predict new data points as they came in, we might see that a model might discover this to be an important indicator and that specific cities have higher rates of fraud."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\nRandomness and Why You Care\n\n\n\n\ncode\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nHayden Brundage\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nHayden Brundage\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHayden Brundage\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nHayden Brundage\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly and Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nHayden Brundage\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "A Linear Classifier"
  },
  {
    "objectID": "posts/Classification/index.html#classification-in-machine-learning",
    "href": "posts/Classification/index.html#classification-in-machine-learning",
    "title": "Classification",
    "section": "Classification in Machine Learning",
    "text": "Classification in Machine Learning\nAlong the same vein of clustering data is a supervised machine learning task known as classification. Classification is a supervised machine learning technique which predicts a ‘category’ for data to belong to based on the analysis of its features. Potential applications of this technique are many, and can range from spam filtering, text sentiment analysis, and even medical diagnosis. Needless to say, in some of these applications it is important to be certain your model produces accurate results and does not suffer from over or underfitting data. In this blog post, I will utilize an ensemble method to determine the a drug to be used on a patient with given medical characteristics."
  },
  {
    "objectID": "posts/Classification/index.html#ensemble-method-for-classification",
    "href": "posts/Classification/index.html#ensemble-method-for-classification",
    "title": "Classification",
    "section": "Ensemble Method for Classification",
    "text": "Ensemble Method for Classification\nAn ensemble machine learning method utilizies multiple machine learning algorithms to yield better predictions than any single constituent algorithm might by itself. In this post, I will be utilizing a Random Forest Classifier to predict the correct drug to be used from data in this dataset. Features range from categorical data like blood pressure and cholesterol to numerical data such as age and plasma sodium-pottasium ratios. Below I display the data and information about each of the features:\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"Drugs.csv\");\n\nprint(df.head());\nprint(\"\\n\");\nprint(df.info());\n\n\n   Age Sex      BP Cholesterol  Na_to_K   Drug\n0   23   F    HIGH        HIGH   25.355  DrugY\n1   47   M     LOW        HIGH   13.093  drugC\n2   47   M     LOW        HIGH   10.114  drugC\n3   28   F  NORMAL        HIGH    7.798  drugX\n4   61   F     LOW        HIGH   18.043  DrugY\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Age          200 non-null    int64  \n 1   Sex          200 non-null    object \n 2   BP           200 non-null    object \n 3   Cholesterol  200 non-null    object \n 4   Na_to_K      200 non-null    float64\n 5   Drug         200 non-null    object \ndtypes: float64(1), int64(1), object(4)\nmemory usage: 9.5+ KB\nNone\n\nFigure 1: ?(caption)\n\n\nWe appear to have no null values in our dataset, which means I do not have to perform any sort of data cleaning before training on it. Additionally, the Random Forest Classifier implementation in scikit-learn requires that we encode our categorical data, which Pandas offers a built-in method for called get_dummies(). After this, we simply train the model on a subset of the dataset and can then test it on the remaining data. Our classifier here creates a number of decision trees, which are ultimately used to create a confusion matrix - a representation of true and false positives and negatives generated when each feature is used as a classifier. The model then uses these matrices and other metrics to determine which features have the strongest and weakest predictive value. In our random forest, a finite number of decision trees (usually defined with the model) ‘vote’ on whether each sample presented is positive or not for some category. For our model below, we will determine the different feature importances and check how accurate our model is:\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\nX = df.drop('Drug', axis='columns')\ny = df['Drug']\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, shuffle=True, random_state=123)\n\nRF_classifier = RandomForestClassifier(n_estimators=300)\nRF_classifier.fit(X_train, y_train)\n\ny_pred = RF_classifier.predict(X_test)\n\nimportance = RF_classifier.feature_importances_\nfeatures = X_test.columns\n\nfeat_importance = pd.Series(importance, index=features).sort_values()\n\nplt.barh(feat_importance.index, feat_importance.values);\nplt.xlabel(\"Relative Feature Importance\");\nplt.ylabel(\"Feature\");\n\nprint(\"Accuracy Score: %s\" %(accuracy_score(y_test, y_pred)))\n\n\nAccuracy Score: 1.0\n\n\n\n\n\nFigure 2: Feature Importances\n\n\n\n\nAn accuracy score of 1.0 means our model was able to perfectly predict drugs based on our test set. Clearly, our random forest model was able to identify some important feature with high predictive value and accurately determine the drug used to treat the patient in question. We can see in the plot above that the sodium-potassium ratio in blood plasma had the highest feature importance. Random forest models determine the importance of each feature during the training process and can either learn over time with more data or maintain static importances. Another important feature of random forests is their relatively low likelihood to overfit data. This comes from their nature as a classifier as they do not often rely directly on statistical quantities to make determinations about the data they make predictions based on."
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "One Dimensional Linear Regression"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#linear-regresssion-models",
    "href": "posts/Linear and Nonlinear Regression/index.html#linear-regresssion-models",
    "title": "Linear and Nonlinear Regression",
    "section": "Linear Regresssion Models",
    "text": "Linear Regresssion Models\nLinear regression represents one of (if not the) simplest class of machine learning models. From a given data set in which there exists one (or many) independent variables, we can perform a regression analysis to generate a linear (or nonlinear - more on that later) predictive model. Using these models, we can make relatively accurate predictions about the value of a dependent variable given the value of a correlated independent variable (or set of independent variables). You have likely performed the simplest of such regressions in a high school math class, where you may have been asked to find the equation of a line that passes through two points. That equation is, in a way, a model that you can use to ‘predict’ the value of some variable \\(y\\) based on the value of the independent variable \\(x\\).\nIn this post, I will be applying the above concepts to a United States Environmental Protection Agency dataset publically available on their website. Their database contains many different datasets pertaining to environmental information, but for this post I have selected a subset of their Air Quality Index (AQI) data. You can find their datasets here. Specifically, this dataset reports ozone concentrations detected in the air at different locations and times in 2022 (data published 11-14-2022). The AQI at each of these locations and times is also reported. There is an obvious correlation between ozone levels and AQI, after all, AQI is essentially a measure of pollution in the air. I would expect, had I not seen the data, that AQI will increase with increased levels of ozone. I do not, however, know the nature of this correlation (be it linear or otherwise), but I expect that performing regression analysis will reveal something about it. In the following code, we store the data in a dataframe and pick a subset of the dataset (specifically, data collected from a site in Washington D.C.) and select the ‘Arithmetic Mean’ column, which is the average concentration of ozone at a given site taken over an hour. I then plot that mean concentration against the measured AQI associated with the given concentration measurement."
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#air-quality-index-and-ozone-concentration",
    "href": "posts/Linear and Nonlinear Regression/index.html#air-quality-index-and-ozone-concentration",
    "title": "Linear and Nonlinear Regression",
    "section": "Air Quality Index and Ozone Concentration",
    "text": "Air Quality Index and Ozone Concentration\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n  \nair_quality = pd.read_csv(\"Ozone_Summary_Data.csv\")\n\ndf = air_quality[air_quality['Local Site Name'] == 'RIVER TERRACE']\n\ndf = df[['Arithmetic Mean', 'AQI']]\n\ndf = df.sort_values('Arithmetic Mean')\n\nplt.scatter(df['Arithmetic Mean'], df['AQI'], s=25)\nplt.xlabel(\"Ozone Concentration (ppm)\")\nplt.ylabel(\"AQI\")\nplt.axis((0, 1.1 * max(df['Arithmetic Mean']), 0, 1.1 * max(df['AQI'])))\nplt.show()\n\n\n\n\n\nFigure 1: AQI Data - Ozone\n\n\n\n\nThe above scatter plot shows the independent variable, ozone concentration in ppm (parts-per-million), against the dependent variable, Air Quality Index (a dimensionless quantity). Examining the data qualita tively leads us to the conclusion that AQI does indeed increase with increased ozone concentration, as one might expect. It appears linear in many aspects, at least in the range displayed. There is plenty we can infer from looking at this data qualitatively, but this is a machine learning blog after all.\nThe data appears linearly correlated, and as such we will choose to perform a linear regression on it. We will use a popular Python data science and machine learning library, scikit-learn, to perform this regression. The specific model used in scikit-learn is called a least squares linear regression, which applies a linear least squares approximation of a function to the data. Essentially, model is created to optimize the square of the error between the value of the dependent variable predicted by the model and its actual value for a given independent variable. If you’ve taken a college-level linear algebra class, this method is usually taught as an important application of algebraic principles. I highly encourage any curious individuals to at least read about the mathematical foundation of this process.\nBelow is is Python code that performs a linear regression on a subset of the data (a “training” set), removes the test data from the dataset, and plots the resulting model along with the rest of the data (the “test” set). Training data should generally be between 70% and 80% of the dataset with the rest used for testing. Additional code is present to display quantities useful for evaluating the model.\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nimport sklearn.metrics as sk\n\n# Get sample data and remove from dataset\naqi_train = df.sample(frac=.75)\naqi_test = df.drop(aqi_train.index)\n\n# Load data to test and training lists\naqi_x_train = aqi_train['Arithmetic Mean']\naqi_x_test = aqi_test['Arithmetic Mean']\n\naqi_y_train = aqi_train['AQI']\naqi_y_test = aqi_test['AQI']\n\n# Load values into NumPy arrays\nx = np.array(aqi_x_train).reshape((-1, 1))\ny = np.array(aqi_y_train)\n\n# Define model and make predictions\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ny_prediction = model.predict(np.array(aqi_x_test).reshape((-1, 1)))\n\n# Plot the data\nplt.scatter(aqi_x_test, aqi_y_test, s=25)\nplt.plot(aqi_x_test, y_prediction, color=\"red\", linewidth=2)\nplt.xlabel(\"Ozone Concentration (ppm)\")\nplt.ylabel(\"AQI\")\nplt.show()\n\nprint(\"Mean Squared Error: %.2f\" % sk.mean_squared_error(aqi_y_test, y_prediction))\nprint(\"Coefficient of Determination: %.2f\" % sk.r2_score(aqi_y_test, y_prediction))\n\n\n\n\n\nFigure 2: A Linear Regression on Ozone-AQI Data\n\n\n\n\nMean Squared Error: 20.11\nCoefficient of Determination: 0.84\n\n\nHere, we have the results of the linear regression superimposed on the test data. We can see the red line appears to take a linear form (\\(y = mx + b\\)). Below the plot, I printed out the Mean Squared Error (MSE) and the Coefficient of Determination (often called the R-squared value). These two metrics tell us a great deal about the accuracy and quality of our model. The MSE is, as the name suggests, the average of the squares of the errors between expected values based on the model and the observed data in the test set. This gives a good idea of how close a regression line is to the data points it was regressed on. In the context of machine learning, we calculate this value from the test set after training on the training data. Out side of an MSE of 0 (perfect model), in isolation, MSE does not give much information about the quality of a model. It does, however, allow us to make comparisons between models where a model might be considered “better” if the MSE of one model is significantly lower than that of another. The other metric, the R-squared value, gives a measure of the proportion of variation in the dependent variable (in our case, AQI) that may be predicted based on the independent variable (ozone concentration)."
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#nonlinear-regression",
    "href": "posts/Linear and Nonlinear Regression/index.html#nonlinear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\nWhile linear correlations may be easily modelled by linear regression, it should come as no surprise that nonlinear correlations require a different type of regression. Fittingly, regression performed to capture nonlinear correlations is called nonlinear regression. This encompasses any nonlinear relationship you could think of including polynomial regression, logistic regression, and exponential regression. In this section, I will capture a nonlinear relationship between an independent and dependent variable. From the same data source as the linear model above, I will pull the PM2.5 dataset, which contains PM2.5 (small particles in the air) concentrations and various other statistics as in the first dataset. As with the first example, I will plot the concentration against AQI values.\n\n\nCode\nair_quality = pd.read_csv(\"PM25_Data.csv\", low_memory=False)\n\ndf = air_quality[air_quality['State Name'] == 'Virginia' ]\n\ndf = df[['Arithmetic Mean', 'AQI']]\n\ndf = df.dropna()\n\ndf = df.sort_values('Arithmetic Mean')\n\nplt.scatter(df['Arithmetic Mean'], df['AQI'], s=25)\nplt.axis((0, 1.1 * max(df['Arithmetic Mean']), 0, 1.1 * max(df['AQI'])))\nplt.xlabel(\"PM2.5 Concentration \" + r'$\\mu$g / $m^3$')\nplt.ylabel(\"AQI\")\nplt.show()\n\n\n\n\n\nFigure 3: AQI Data - PM2.5\n\n\n\n\nQualitatively, the data appears to be quite clustered and we see that AQI increases with the PM2.5 concentration. The data appears somewhat linear for low values, but appears to curve slightly before continuing to ascend, albeit slower. A linear model would have poor performance modelling this data and poor predictive value, so I will select a nonlinear model. We will perform a polynomial regression on the data to generate a model here, specifically with a degree of two. Performing this regression will generate a best-fit line to the data of the form \\(y = ax^2 + bx + c\\). The same metrics will be applicable for model evaluation here as were for the linear model earlier. As with the first model, the following Python code takes a random sample from the dataset and trains the model before using it to predict values from the reserved test data. The predicted values are then compared to the actual values in the test set and the model is evaluated for its quality and accuracy. Below is commented code and the produced graph:\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Get sample data and remove from dataset\naqi_train = df.sample(frac=.75)\naqi_test = df.drop(aqi_train.index)\n\n# Load data to test and training lists\naqi_x_train = aqi_train['Arithmetic Mean']\naqi_x_test = aqi_test['Arithmetic Mean']\n\naqi_y_train = aqi_train['AQI']\naqi_y_test = aqi_test['AQI']\n\n# Load values into NumPy arrays\nx_train = np.array(aqi_x_train).reshape((-1, 1))\ny_train = np.array(aqi_y_train)\n\nx_test = np.array(aqi_x_test).reshape((-1, 1))\ny_test = np.array(aqi_y_test)\n\n# Define model and transform data\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nx_poly = poly_features.fit_transform(x_train)\nx_test = poly_features.transform(x_test)\n\npoly_model = LinearRegression()\npoly_model.fit(x_poly, y_train)\n\n# Predict outcome\ny_pred = poly_model.predict(x_test)\n\n# Plot the data (we create data here to make the graph prettier)\nx_graph = np.linspace(1, 40, num=39).reshape((-1, 1))\nx_graph_poly = poly_features.transform(x_graph)\ny_graph = poly_model.predict(x_graph_poly)\n\nplt.scatter(aqi_x_test, aqi_y_test, s=25)\nplt.plot(x_graph, y_graph, color='red', linewidth=2)\nplt.axis((0, 1.1 * max(df['Arithmetic Mean']), 0, 1.1 * max(df['AQI'])))\nplt.xlabel(\"PM2.5 Concentration \" + r'$\\mu$g / $m^3$')\nplt.ylabel(\"AQI\")\nplt.show()\n\nprint(\"Mean Squared Error: %.2f\" % sk.mean_squared_error(y_test, y_pred))\nprint(\"Coefficient of Determination: %.2f\" % sk.r2_score(y_test, y_pred))\n\n\n\n\n\nFigure 4: A Polynomial Regression on PM2.5-AQI Data\n\n\n\n\nMean Squared Error: 0.48\nCoefficient of Determination: 1.00\n\n\nAs we can see, the model fits the data extremely well and the MSE and r-squared values show that the model is quite good at accurately predicting the dependent variable. Depending on the random sample taken, there may be outlier values toward the higher end of the model. I encourage you to run the code several times until this behavior is seen. This shows a limitation common to regression methods: models may not be excellent at capturing the relationship between two features if the dataset does not adequately capture their relationship in a well-distributed manner of the range of data. The r-squared value is likely 1.00 or very close to 1.00 every time the code is ran and thus suggests the dependent variable is extremely predictable based on the independent variable. Similarly, the MSE for the model is likely very close to 0.00, which implies the data is clustered very closely to what the model predicts.\nIf you are familiar with the effects of PM2.5, you may know that the effect it has on AQI is significant even at low concentrations. As the data and resulting model show, there is a very strong, almost linear relationship at lower concentrations of PM2.5 particles. From my own research, I knew that PM2.5 has a diminshing effect (i.e. lower increase in AQI) as concentration increases. While this is somewhat adequately captured by the model, the relative lack of data at high concentrations prevents the model from capturing this adequately. A logarithmic function may have better modelled this data, but in any case, it is clear that a nonlinear model fits the data and predicts outcomes better than a linear model would have. Performing a linear regression on the data and comparing the MSE and r-squared values is a great follow up activity after reading this post."
  }
]